# Evaluation

Several metrics have been defined to compare the models used in our project work. We will focus on reconstruction loss during the training. We will define the reconstruction loss in terms of the mean squared error (MSE) of the obtained and expected values.

$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2$$

## About the baseline model
We have chosen the variational autoencoder (VAE) as our baseline model, as we have already implemented it in our previous semester for generating handwritten numbers based on the MNIST database. In the papers we have read, this construct has been overshadowed by diffusion models, so we also thought we would investigate how much difference there will be between the images generated by the two approaches.

The VAE model will be implemented by us based on our existing knowledge, but we do not aim to create a fully optimised one. We want to train the model on the CelebA and Flowers102 datasets we use. We will not implement hyperparameter optimization during the training, but we will use the best model based on the reconstruction loss for the comparison. During training we will use EarlyStopping to prevent over-learning, as we will be testing on a completely independent subset, so we do not want to be able to map only the images that are included in the training to the latent space.

## More specific metrics

**[Inception Score](https://en.wikipedia.org/wiki/Inception_score)** or **[Frechet Inception Distance (FID)](https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance)** is most commonly used to compare the generated images. The Inception Score measures how well a model generates realistic, distinguishable images by examining both the clarity and variability of generated samples. The FID measures how similar the features of generated images are to those of real images, with smaller values implying more realistic and high-quality generated images.

$$IS = exp(\mathbb{E}_x KL(p(y | x ) \Vert\ p(y)))$$

$$ FID = \|\mu - \mu_w\|^2 + tr(\Sigma + \Sigma_w - 2(\Sigma \Sigma_w)^{\frac{1}{2}}) $$

In evaluating models, a lower FID score indicates a closer match between the generated and real data distributions. A higher Inception Score means that the generated images are of very high quality and diversity. 

For more accurate measurements, the Inception model behind the IS and FID values could be fine-tuned for the datasets we use (CelebA, Flowers102), but we will ignore this step for now. We will use the [InceptionScore](https://lightning.ai/docs/torchmetrics/stable/image/inception_score.html) and [FrechetInceptionDistance](https://lightning.ai/docs/torchmetrics/stable/image/frechet_inception_distance.html) implementations built into the Pytorch Lightning module with the base model weights. This will introduce some inaccuracy into the result, but probably not too much. If it does cause a problem, we plan to correct this before final submission.

## The evaluation process

For the objective measurements, we will use a previously developed test data set. We plan to use it to measure the reconstruction loss and to investigate the quality of the images generated by the models compared to the original images, using the IS and FID values mentioned above. When determining the IS and FID values, we have to take into account that the resolution of the images we will use are 64×64, while the Inception V3 model works with 299×299 images. For this reason we need to scale up both the original images and the generated images. For scaling, we plan to use Pytorch's transform.Resize transform using [Bicubic](https://en.wikipedia.org/wiki/Bicubic_interpolation) interpolation mode, as this can guarantee the best quality. Of course, scaling up will also affect the accuracy of the measured result.

We expect that our diffusion model will produce better reconstruction loss, IS and FID for both datasets. 

As a subjective comparison, two approaches are envisaged. In the first, for the first $n$ images selected from the test dataset, we generate what the models return and compare how the generated images compare visually to the original image. We then plan to generate the same number of images by both models, taking into account that in the case of the VAE we have to generate a random vector in the latent representation, while in the case of the diffusion model we have to generate a random noise.

## Final thoughts

By the end of the project, we aim to have a model that is significantly better than the VAE model used as a baseline model. It is expected that we will only be able to approximate the results reported in the articles referenced in [README.md](https://github.com/Nemes2000/diffusion-model/blob/main/README.md), but we will strive to produce as good a model as possible.
